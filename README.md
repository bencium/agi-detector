# AGI Detector üß†

[![Build Status](https://img.shields.io/badge/build-in%20progress-yellow)](https://github.com/bencium/agi-detector)
[![Next.js](https://img.shields.io/badge/Next.js-14-black)](https://nextjs.org/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

An advanced monitoring system designed to detect early signs of Artificial General Intelligence (AGI) emergence by analyzing patterns across AI research labs, academic papers, and technology news.

## üéØ Project Vision

A sophisticated early-warning system that monitors the AI landscape for genuine indicators of AGI development, distinguishing between normal AI progress and potentially groundbreaking developments that could signal the emergence of AGI.

**Concept and AI Agent Orchestration by [Bence Csernak ‚Ä¢ bencium.io](https://www.bencium.io)**

## üöÄ Key Features

### Core Monitoring
- **Multi-Source Crawling**: Successfully monitors **40+ sources** across academic, social, code repositories, and traditional media:
  - **Academic**: arXiv (4 categories), Semantic Scholar (200M papers), Papers with Code (5000+ benchmarks), Hugging Face (models + datasets + leaderboards)
  - **Social**: Reddit (5 AI/ML subreddits - optional)
  - **Code**: GitHub (trending repos + 20 key organizations - optional)
  - **Traditional**: OpenAI, DeepMind, Anthropic, Microsoft AI, TechCrunch, VentureBeat
- **GPT-5-mini Powered Analysis**: Uses cost-efficient GPT-5-mini model for sophisticated AGI indicator detection
- **Keyword Heuristic Pre-Filtering**: Aggressive filtering reduces false positives by 50-70% before LLM analysis
- **Accuracy Metrics System**: Quantified tracking of precision, recall, F1 score, and confusion matrix
- **Real-time Console Output**: Detailed logging shows every article being analyzed, batch progress, and errors
- **Enhanced Processing Indicators**: Glowing button animation and automatic console expansion during analysis
- **Real-time Alerts**: Automated detection and severity classification (none/low/medium/high/critical)
- **Cross-Validation**: Independent verification system that only increases scores (never decreases) to ensure no AGI progress is hidden

### Advanced Analytics
- **Historical Tracking**: Complete database of all analyses with trend data
- **Trend Visualization**: Interactive charts showing AGI risk trends over time
- **Risk Assessment**: Daily, weekly, and monthly trend analysis
- **Pattern Recognition**: Identifies unusual spikes or accelerating trends
- **Accuracy Metrics**: Precision/recall/F1 tracking with ground truth validation system
- **Author Prestige Scoring**: H-index analysis, institutional affiliations, publication venue detection
- **Benchmark Detection**: Automatic extraction of 11 AGI benchmark results (MMLU, HumanEval, GAIA, etc.)
- **Citation Velocity Analysis**: Papers with Code and Semantic Scholar integration
- **Multilingual Support**: Language detection for 11 languages with priority boosting

### Enhanced Detection System
- **Comprehensive AGI Indicators**:
  - Recursive self-improvement capabilities
  - Novel algorithm creation
  - Cross-domain generalization
  - Emergent capabilities from scale
  - Meta-learning breakthroughs
  - Autonomous research abilities
  - Human-level performance on complex tasks
  - Reasoning and planning breakthroughs
  - Self-awareness indicators
  - Generalization leaps
- **Near-term AGI Detection**: Also monitors architectural innovations, benchmark improvements (>10%), multi-modal capabilities, tool use, and chain-of-thought reasoning
- **Improved Scoring System**:
  - 0.0-0.1: No AGI relevance
  - 0.1-0.3: Minor AI advancements
  - 0.3-0.5: Significant AGI progress
  - 0.5-0.7: Major breakthrough
  - 0.7-1.0: Critical AGI development
- **Evidence Quality Rating**: Classifies findings as speculative, circumstantial, or direct
- **Cross-Reference System**: Suggests additional sources for verification

### False Positive Reduction System (Phase 3)
- **Keyword Heuristic Pre-Filtering**:
  - Analyzes content before expensive LLM calls
  - Detects marketing hype, vague claims, and incremental improvements
  - **50-70% rejection rate** for low-quality content
  - Reduces API costs by filtering noise
- **Media Noise Detection**:
  - Identifies sensationalist language patterns
  - Penalizes marketing buzzwords without evidence
  - Filters clickbait and vague "approaching AGI" claims
- **Technical Signal Boosting**:
  - Prioritizes content with code, math, benchmarks
  - Boosts papers from prestigious venues (NeurIPS, ICML, Nature)
  - Increases weight for high-prestige authors (h-index >50)
- **Ground Truth Validation**:
  - Expert labeling system for continuous improvement
  - Confusion matrix tracking (TP/FP/TN/FN)
  - Target: <10% false positive rate (from baseline 20-40%)

### NLP & Text Processing (Phase 3)
- **Lightweight NLP Preprocessing**:
  - Regex-based processing (no heavy dependencies like spaCy)
  - Sentence splitting, tokenization, stop word removal
  - Technical density scoring (code/math detection)
  - Entity extraction (organizations, benchmarks, models)
  - Readability metrics (Flesch-Kincaid approximation)
  - Unicode quote normalization for international text
- **Feature Extraction**:
  - Code block detection (markdown, function definitions)
  - Math content detection (LaTeX, symbols, mathematical terms)
  - URL and citation detection
  - Average sentence length and word count
  - Technical vs. marketing language classification

### Beautiful UI
- **Anthropic-Inspired Design**: Clean, minimal interface with sophisticated aesthetics
- **Four-Tab Navigation**: Overview, Findings, Analysis, and Trends
- **Enhanced Risk Level Indicator**: Shows current risk level with details (average score, critical findings count)
- **Real-time Console Output**: Floating console window shows detailed analysis progress
- **Processing Animations**: Glowing button effect and automatic console expansion during analysis
- **Validation UI**: Clear "Get 2nd opinion" button with detailed summary dialog
- **Source Status Cards**: Shows article counts and working status for each source
- **Responsive Layout**: Works seamlessly on desktop and mobile devices

## üèóÔ∏è Architecture

### Tech Stack
- **Frontend**: Next.js 14 + React 19 + TypeScript
- **Styling**: TailwindCSS with custom Anthropic-inspired theme
- **Database**: PostgreSQL (via Neon) + Prisma ORM
- **AI Integration**: OpenAI GPT-4.1
- **Web Crawling**: 
  - Advanced multi-strategy crawler with RSS feed support
  - Playwright browser automation for JavaScript-heavy sites
  - User agent rotation and stealth techniques
  - Rate limiting (1 request per 2 seconds)
- **Data Visualization**: Custom SVG trend charts

### Database Schema
```prisma
- CrawlResult: Stores crawled articles
- AnalysisResult: Enhanced with severity, evidence quality, cross-references, ground truth labels, embeddings, language detection, heuristic scores, media noise scores
- AccuracyMetrics: Tracks precision, recall, F1 score, confusion matrix (TP/FP/TN/FN)
- SourceMetrics: Monitors source reliability and performance
- HistoricalData: Tracks metrics over time
- TrendAnalysis: Aggregated trend data for visualization
```

## üõ†Ô∏è Installation

### Prerequisites

#### **REQUIRED** (Core Functionality):

1. **OpenAI API Key**
   - Sign up at [platform.openai.com](https://platform.openai.com)
   - Go to API Keys section and create a new key
   - Free tier includes $5 credit (enough for ~1000 analyses)
   - Note: You'll need to add payment method for continued use

2. **PostgreSQL Database** (Choose one):
   - **[Neon](https://neon.tech)** (Recommended - Free tier)
     - Sign up and create a new project
     - Copy the connection string from the dashboard
     - Free tier: 3GB storage, perfect for this project
   - **[Supabase](https://supabase.com)** (Alternative)
     - Create new project and get the connection string
     - Free tier: 500MB storage
   - **Local PostgreSQL**
     - Install PostgreSQL locally
     - Create a database: `createdb agi_detector`
     - Connection string: `postgresql://localhost/agi_detector`

#### **OPTIONAL** (Enhanced Features - All Free):

3. **GitHub Token** (Increases rate limit 60‚Üí5000 req/hr)
   - Go to [github.com/settings/tokens](https://github.com/settings/tokens)
   - Generate new token (classic) with `public_repo` scope
   - Free, no payment required

4. **Reddit Credentials** (Enables 5 subreddits monitoring)
   - Go to [reddit.com/prefs/apps](https://www.reddit.com/prefs/apps)
   - Create a "script" app
   - Get Client ID and Secret
   - Free, no payment required

5. **Brave Search API** (Advanced web crawling)
   - Get free tier at [brave.com/search/api](https://brave.com/search/api/)
   - Free tier available

6. **Firecrawl API Key** (For DeepMind & Anthropic)
   - Sign up at [firecrawl.dev](https://www.firecrawl.dev)
   - Free tier: 500 credits/month

### Installation Steps

1. **Clone the repository**:
```bash
git clone https://github.com/bencium/agi-detector.git
cd agi-detector
```

2. **Install dependencies**:
```bash
npm install --legacy-peer-deps

# Install Playwright browsers for advanced crawling
npx playwright install chromium
```

3. **Set up environment variables**:
Create a `.env.local` file with exactly these variables:
```env
# ===== REQUIRED =====
# PostgreSQL Database
DATABASE_URL="postgresql://user:password@host/database?sslmode=require"
# Optional direct connection for migrations:
DIRECT_URL="postgresql://user:password@host/database?sslmode=require"

# OpenAI API Key (REQUIRED)
API_KEY=sk-...
# or
OPENAI_API_KEY=sk-...

# ===== OPTIONAL (Enhanced Features) =====
# GitHub Token (increases rate limit 60‚Üí5000 req/hr)
GITHUB_TOKEN=ghp_...

# Reddit Credentials (enables 5 subreddits monitoring)
REDDIT_CLIENT_ID=your_14_char_id
REDDIT_CLIENT_SECRET=your_secret_here
REDDIT_USER_AGENT=AGI-Detector-Bot/1.0

# Brave Search API (advanced web crawling)
BRAVE_API_KEY=BSA...

# Firecrawl API Key (for DeepMind and Anthropic)
FIRECRAWL_API_KEY=fc-...

# Model override (defaults to gpt-5-mini)
OPENAI_MODEL=gpt-5-mini

# Proxy for web crawling
PROXY_URL="http://your-proxy:port"

# Validation UI thresholds (client-side)
NEXT_PUBLIC_VALIDATION_MIN_SEVERITY=medium  # none|low|medium|high|critical
NEXT_PUBLIC_VALIDATION_ALWAYS=false         # show Validate for all analyses
```

**Important Notes about .env.local**:
- Replace `DATABASE_URL` with your actual PostgreSQL connection string
- Replace `API_KEY` with your OpenAI API key (no quotes needed)
- All GitHub, Reddit, Brave, Firecrawl keys are **optional** (free sources work without them)
- With ONLY OpenAI + Database: 30+ sources work (arXiv, Semantic Scholar, Papers with Code, Hugging Face, traditional web)
- Add GitHub + Reddit for full 40+ source coverage
- The format must be exactly as shown (no extra quotes around keys)

4. **Set up the database**:
```bash
npx prisma generate
npx prisma db push
```

5. **Run the development server**:
```bash
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) to view the application.

### üéØ Quick Start Tips

1. **First Run**:
   - Click "Run Manual Scan" to crawl all sources
   - With ONLY OpenAI + Database: 30+ sources work immediately (arXiv, Semantic Scholar, Papers with Code, Hugging Face, traditional web)
   - Add GitHub + Reddit credentials for full 40+ source coverage
   - The system will automatically analyze articles for AGI indicators
   - Check the console output at the bottom for progress

2. **Cost Optimization**:
   - The system uses GPT-5-mini which is very cost-efficient
   - **Keyword heuristic pre-filtering reduces API costs by 50-70%**
   - Each analysis costs ~$0.001 (1000 analyses ‚âà $1)
   - All academic sources are free (no API keys needed)
   - Firecrawl free tier is sufficient for DeepMind + Anthropic crawling

3. **Source Coverage**:
   - **Free (no credentials)**: 30+ sources (arXiv, Semantic Scholar, Papers with Code, Hugging Face, 7 traditional sources)
   - **+GitHub token**: Add trending repos, 20 key organizations, star velocity
   - **+Reddit credentials**: Add 5 AI/ML subreddits monitoring
   - **Total**: 40+ sources with all optional credentials

4. **Troubleshooting**:
   - If sources show 0 articles, check your Firecrawl API key (DeepMind/Anthropic only)
   - Database connection errors: Verify your DATABASE_URL format
   - OpenAI errors: Ensure your API key has credits
   - GitHub/Reddit not working: Check token/credentials in .env.local

### üîí Security Notes

**CRITICAL: Protect Your API Keys**
- ‚ö†Ô∏è **NEVER** commit `.env` or `.env.local` files to version control
- ‚ö†Ô∏è Keep your OpenAI, database, and other API keys secure and private
- ‚ö†Ô∏è For production deployments, use environment variables from your hosting platform

**Security Features (October 2025 Update):**
- ‚úÖ SSRF protection - blocks localhost, private IPs, and cloud metadata endpoints
- ‚úÖ Safe JSON parsing - prevents crashes from malformed responses
- ‚úÖ Input validation - all API parameters validated with Zod schemas
- ‚úÖ Request limits - 1MB body size, 10MB response size, 30s timeouts
- ‚úÖ Security headers - X-Frame-Options, XSS protection, content-type sniffing protection
- ‚úÖ Browser security - Playwright runs without dangerous --disable-web-security flags
- ‚úÖ SQL injection protection - Prisma ORM with parameterized queries
- ‚úÖ Regular dependency updates - automated security patch management

**Best Practices for Users:**
1. **API Keys**: Use separate keys for development/production, rotate regularly
2. **Network**: Only crawl trusted public sources (default configuration is safe)
3. **Updates**: Run `npm update` and `npm audit fix` regularly for security patches
4. **Monitoring**: Watch for unusual API usage or costs
5. **Local Only**: This tool is for local use; don't expose APIs publicly without authentication

**For Security Issues:**
See [SECURITY.md](SECURITY.md) for our security policy and how to report vulnerabilities responsibly.

### üóÑÔ∏è Database (Prisma) Notes
- If `DATABASE_URL` is not set, the server runs in a no-DB mode. API routes like `/api/data` and `/api/trends` return empty arrays so you can exercise the UI and crawling/analysis flows without persistence.
- To enable persistence locally:
  1) Set `DATABASE_URL` in `.env.local`.
  2) Run `npx prisma generate`.
  3) Run `npx prisma db push` (or `npx prisma migrate dev`).

## üï∑Ô∏è Web Crawling Implementation

### Advanced Crawling Features
The AGI Detector uses a sophisticated multi-strategy crawling system to bypass blocking and collect data from various sources:

#### Crawling Strategies (in order of attempt):
1. **RSS Feed Parsing**: Primary method for most sources
   - Fastest and most reliable
   - No blocking issues
   - Structured data format

2. **Advanced HTTP Requests**: Fallback with stealth headers
   - User agent rotation
   - Browser-like headers
   - Cookie support
   - Automatic decompression

3. **Brave Web Search (New)**: Site-restricted fallback via Brave Search API
   - Query form: `site:<host> [source-specific keywords]`
   - Normalizes results to article candidates (title, snippet, URL)
   - Honors a short TTL cache and minimal rate-limiting
   - Requires `BRAVE_API_KEY`

4. **Firecrawl API**: For heavily protected sources
   - Professional web scraping service
   - Handles JavaScript rendering
   - Bypasses most anti-bot measures
   - Limited to 50 requests/day (cached for 24h)

5. **Browser Automation**: Last resort using Playwright
   - Full JavaScript rendering
   - Stealth plugins to avoid detection
   - Random viewport sizes
   - Human-like delays

#### Working Sources (40+ Total):

**Traditional Media (7 sources):**
- ‚úÖ **OpenAI Blog**: ~500 articles via RSS feed
- ‚úÖ **DeepMind Research**: ~100 articles via advanced crawling
- ‚úÖ **Anthropic Blog**: 3+ articles via Firecrawl API
- ‚úÖ **Microsoft AI Blog**: ~30 articles via RSS feed
- ‚úÖ **TechCrunch AI**: ~60 articles via RSS feed
- ‚úÖ **VentureBeat AI**: ~90 articles via RSS feed
- ‚úÖ **arXiv AI**: ~200 papers via HTML parsing (4 categories: cs.AI, cs.LG, cs.CL, cs.CV)

**Academic Sources (3 sources - FREE APIs):**
- ‚úÖ **Semantic Scholar**: 200M papers, citation velocity analysis (free API, no auth)
- ‚úÖ **Papers with Code**: 5000+ benchmarks, SOTA tracking (free API, no auth)
- ‚úÖ **Hugging Face**: Trending models, AGI datasets, Open LLM Leaderboard (free API, no auth)

**Social Sources (5 sources - OPTIONAL):**
- ‚úÖ **Reddit**: r/MachineLearning, r/artificial, r/singularity, r/ControlProblem, r/mlscaling (requires credentials)

**Code Repositories (20+ sources - OPTIONAL):**
- ‚úÖ **GitHub**: Trending AI repos, 20 key organizations (OpenAI, DeepMind, Anthropic, etc.), star velocity (requires token)

### Crawler Configuration
The crawler includes:
- **Rate Limiting**: 1 request per 2 seconds
- **Random Delays**: 2-5 seconds between requests
- **User Agent Pool**: Rotates through real browser user agents
- **Error Handling**: Automatic retries with exponential backoff
- **Resource Cleanup**: Proper browser instance management

## üìä Usage Guide

### Manual Monitoring
1. Click "Run Manual Scan" to immediately crawl all sources
2. The system will automatically analyze findings for AGI indicators
3. View results in the "Analysis" tab with severity ratings

### Automated Monitoring
1. Click "Start Monitoring" to enable daily automated crawls
2. The system checks every hour and runs a crawl every 24 hours
3. Critical findings trigger immediate alerts

### Trend Analysis
1. Navigate to the "Trends" tab
2. Select time period: Daily, Weekly, or Monthly
3. View risk trends, milestones, and historical patterns

### Validation System
1. Articles marked for verification show a "Validate" button with "Get 2nd opinion" text
2. Click to run independent AI analysis that:
   - Never decreases scores (only increases)
   - Finds additional AGI indicators
   - Increases confidence levels
   - Cross-checks with suggested sources
3. Shows detailed summary with:
   - Score changes (e.g., 5% ‚Üí 15%)
   - Confidence changes
   - New indicators found
   - AI reasoning

## üîß API Endpoints

- `POST /api/crawl` - Trigger manual crawl of all sources
- `POST /api/analyze-all` - Analyze all unprocessed articles (processes in batches of 50)
- `GET /api/data` - Get all crawled articles and analyses with source statistics
- `GET /api/trends?period={daily|weekly|monthly}` - Fetch trend data
- `POST /api/validate` - Validate specific analysis results (only increases scores)
- `POST /api/test-crawler` - Test individual crawler sources
 - `POST /api/backfill-historical` - One-off backfill of HistoricalData from existing analyses
 - `POST /api/backfill-severities` - One-off recompute of severities from scores (escalate-only)
- `GET /api/db-health` - Quick DB connectivity and counts (crawl/analysis/trends)
  ```json
  {
    "source": {
      "name": "OpenAI Blog",
      "url": "https://openai.com/blog",
      "selector": ".ui-list__item",
      "titleSelector": ".ui-title-text",
      "contentSelector": ".prose",
      "linkSelector": "a"
    }
  }
  ```

## üß™ Testing

Run the test suite:
```bash
npm test
```

**Test Coverage: 100%** (55/55 tests passing)
- ‚úÖ Keyword heuristics: 8 tests
- ‚úÖ Accuracy metrics: 6 tests
- ‚úÖ Benchmark detector: 8 tests
- ‚úÖ Author prestige: 11 tests
- ‚úÖ NLP preprocessing: 16 tests
- ‚úÖ API routes: 2 tests
- ‚úÖ Components: 1 test
- ‚úÖ Utilities: 3 tests

Watch mode for development:
```bash
npm run test:watch
```

For crawl testing:
```bash
# Test all sources
curl -X POST http://localhost:3000/api/crawl | jq

# Test specific source
curl -X POST http://localhost:3000/api/test-crawler \
  -H "Content-Type: application/json" \
  -d '{
    "source": {
      "name": "OpenAI Blog",
      "url": "https://openai.com/blog",
      "selector": ".ui-list__item",
      "titleSelector": ".ui-title-text",
      "contentSelector": ".prose",
      "linkSelector": "a"
    }
  }' | jq
```

## üßØ Troubleshooting: Analyze-All Hangs / Timeouts

If clicking "Run Manual Scan" leads to a long spinner and no results, it‚Äôs often due to the analysis phase (`POST /api/analyze-all`) waiting on external services. We‚Äôve hardened the endpoint to avoid indefinite hangs and added tunable timeouts.

What typically causes the delay
- OpenAI API timeouts during batch analysis (most common). You‚Äôll see logs like:
  - `[Analyze] Error type: APIConnectionTimeoutError` and `Request timed out.`
- Database latency or connection issues when reading/writing analyses.

How it‚Äôs mitigated in code
- Per‚Äëstep timeouts and fast‚Äëfail handling in `src/app/api/analyze-all/route.ts`:
  - DB reads/writes: 10‚Äì15s cap with clear log messages.
  - OpenAI analysis: explicit per‚Äërequest timeout and no automatic retries.
  - Batch processing: `Promise.allSettled` with a per‚Äëbatch timeout so one slow item doesn‚Äôt stall the whole request.
- On timeout, the API returns HTTP 504 with an array of `logs` instead of hanging.

Environment tuning (recommended for dev)
- Add these to `.env.local` to speed up responses and reduce timeouts:
  - `OPENAI_MODEL=gpt-4o-mini` (fast, reliable)
  - `OPENAI_TIMEOUT_MS=12000` (per‚Äërequest timeout)
  - `ANALYZE_BATCH_SIZE=2` (lower concurrency)
  - `BATCH_TIMEOUT_MS=18000` (per‚Äëbatch cap)

Quick diagnostics
- Check OpenAI reachability:
  - `curl -s http://localhost:3000/api/test-openai` should return a short message.
- Check DB health:
  - `curl -s http://localhost:3000/api/db-health` should be 200 with counts; 503 means no DB configured.
- Exercise analyze-all with a client timeout:
  - `curl -i -m 45 -X POST http://localhost:3000/api/analyze-all`
  - Expect 200 with results or 504 within ~20‚Äì40s with a JSON body including `logs`.

Reading the last log line
- Last line is ‚ÄúAbout to query database‚Ä¶‚Äù ‚Üí DB `findMany` is stalling (pooling/network/statement timeout).
- Shows ‚ÄúFound X unanalyzed articles‚Äù then ‚ÄúProcessing batch ‚Ä¶‚Äù with repeated OpenAI timeouts ‚Üí reduce `ANALYZE_BATCH_SIZE`, increase `OPENAI_TIMEOUT_MS`, or switch model.
- Timeouts during `DB create(analysisResult)`/`createMany(historicalData)` ‚Üí investigate DB write latency; consider Neon pooled connection and statement timeout.

Notes
- The crawl step (`POST /api/crawl`) can be long, but it now runs independent of analysis timeouts. The UI triggers analyze-all after crawling; with the settings above, analysis should no longer block indefinitely.

## ‚ö†Ô∏è Important Notes

### Security
- Never commit `.env.local` to version control
- Rotate API keys regularly
- Use environment variables from your hosting platform in production

### Rate Limiting
- Crawler implements 2-5 second delays between requests
- OpenAI API calls are rate-limited to 1 per second
- Advanced rate limiter using token bucket algorithm
- Respects robots.txt via proxy configuration

### Crawler Dependencies
The advanced crawler requires:
- `playwright`: Browser automation
- `limiter`: Rate limiting
- `user-agents`: User agent rotation
- `xml2js`: RSS feed parsing

### Data Accuracy & Privacy
- This is an experimental system for research purposes
- Should not be relied upon as the sole indicator of AGI emergence
- Designed to supplement, not replace, expert analysis
- Article content is sent to OpenAI for analysis - review OpenAI's data policy
- URLs are accessed via Firecrawl and Brave Search APIs when configured
- All data stays in your local database; no telemetry or external reporting

## üöÄ Deployment

### Vercel (Recommended)
1. Push to GitHub
2. Import project in Vercel
3. Add environment variables
4. Deploy

### Docker
```bash
docker build -t agi-detector .
docker run -p 3000:3000 --env-file .env.local agi-detector
```

## ü§ù Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìù License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üë§ Author

**Bence Csernak**
- Website: [bencium.io](https://bencium.io)
- GitHub: [@bencium](https://github.com/bencium)
- Concept and AI Agent Orchestration: [www.bencium.io](https://www.bencium.io)

## üôè Acknowledgments

- OpenAI for GPT-4 API access
- Anthropic for UI design inspiration
- All monitored AI research labs for their transparency
- Contributors and testers

## üÜï Recent Updates (November 2025)

### v3.0 ‚Äî Phase 3: Advanced Detection & False Positive Reduction (November 17, 2025)
**Major enhancements for accuracy, multi-source integration, and cost optimization:**

#### üéØ False Positive Reduction System
- **Keyword Heuristic Pre-Filtering**: Aggressive filtering rejects 50-70% of low-quality content before LLM analysis
  - Marketing hype detection (penalizes "revolutionary", "game-changing" without evidence)
  - Vague claim filtering ("approaching AGI", "steps toward AGI" without specifics)
  - Incremental improvement rejection (<5% benchmark gains)
  - Media noise scoring with automatic rejection threshold
- **Cost Savings**: 50-70% reduction in OpenAI API calls through pre-filtering
- **Target**: <10% false positive rate (from baseline 20-40%)

#### üìä Accuracy Metrics System
- **Quantified Performance Tracking**:
  - Precision, recall, F1 score calculation
  - Confusion matrix (TP/FP/TN/FN) tracking
  - Daily, weekly, monthly, and all-time aggregation
  - Ground truth validation system for expert labeling
- **New Database Models**: `AccuracyMetrics` and `SourceMetrics`
- **Comprehensive Testing**: 6 unit tests for metrics calculation and reporting

#### üåê Multi-Source Integration (40+ Sources)
- **Hugging Face Integration** (NEW):
  - Trending AI models monitoring
  - AGI-relevant datasets tracking
  - Open LLM Leaderboard integration
  - 8 AGI-specific model tags
- **Semantic Scholar**: 200M academic papers, citation velocity analysis (free API)
- **Papers with Code**: 5000+ benchmarks, SOTA tracking (free API)
- **GitHub**: Trending repos, 20 key organizations, star velocity (optional token)
- **Reddit**: 5 AI/ML subreddits monitoring (r/MachineLearning, r/artificial, etc.) (optional credentials)
- **arXiv Expansion**: 4 categories (cs.AI, cs.LG, cs.CL, cs.CV) from 1
- **Traditional Sources**: OpenAI, DeepMind, Anthropic, Microsoft AI, TechCrunch, VentureBeat

#### üèÜ Author Prestige Scoring (NEW)
- **Prestige Analysis**:
  - H-index scoring (world-class >100, influential >50, established >20)
  - Institutional affiliation detection (Tier 1: OpenAI, DeepMind, MIT, Stanford, etc.)
  - Publication venue ranking (NeurIPS, ICML, ICLR, Nature, Science)
  - Notable researcher recognition (Hinton, Bengio, LeCun, etc.)
- **Boost Multipliers**: 1.0x-2.0x based on prestige score
- **11 Unit Tests**: Comprehensive coverage of prestige calculation

#### üìù Lightweight NLP Preprocessing (NEW)
- **Regex-Based Processing** (no heavy dependencies like spaCy):
  - Sentence splitting and tokenization
  - Stop word removal
  - Technical density scoring
  - Entity extraction (organizations, benchmarks, model names)
  - Readability metrics (Flesch-Kincaid approximation)
- **Feature Extraction**:
  - Code block detection (markdown, function definitions)
  - Math content detection (LaTeX, symbols, terms)
  - URL and citation detection
  - Unicode quote normalization for international text
- **16 Unit Tests**: Comprehensive NLP functionality coverage

#### üîç Benchmark Detection (NEW)
- **Automatic Extraction**: 11 AGI benchmarks tracked
  - MMLU, HumanEval, GAIA, AGIEval, MMMU, MATH, GSM8K, ARC-AGI, GPQA, SWE-Bench, Big-Bench
- **Human-Level Detection**: Automatic flagging of ‚â•95% performance
- **Significance Scoring**: Weighted by benchmark importance
- **8 Unit Tests**: Pattern matching and result extraction

#### üåç Multilingual Support (NEW)
- **Language Detection**: 11 languages supported
  - English, Chinese, German, French, Japanese, Spanish, Russian, Korean, Italian, Portuguese, Arabic
- **Priority Boosting**: Chinese and German research prioritized
- **Translation Flagging**: Identifies content needing translation

#### üß™ Testing Excellence
- **55/55 Tests Passing** (100% success rate)
- **New Test Suites**:
  - Keyword heuristics: 8 tests
  - Accuracy metrics: 6 tests
  - Benchmark detector: 8 tests
  - Author prestige: 11 tests
  - NLP preprocessing: 16 tests

#### üóÑÔ∏è Database Enhancements
- **Extended `AnalysisResult` Schema**:
  - `groundTruthLabel`, `groundTruthScore`, `reviewedAt`
  - `embedding` (for future semantic search)
  - `language` (detected language code)
  - `filteredByHeuristics`, `heuristicScore`, `mediaNoiseScore`
  - `containsCode`, `containsMath`
- **New Models**: `AccuracyMetrics`, `SourceMetrics`

#### üí∞ Cost Optimization
- **No Paid APIs Required**: All free tier services (Semantic Scholar, Papers with Code, Hugging Face)
- **50-70% API Cost Reduction**: Through heuristic pre-filtering
- **Lightweight Dependencies**: Avoided spaCy (450MB+) in favor of regex-based NLP
- **Optional Enhancements**: GitHub and Reddit tokens are free but optional

#### üìö Documentation
- **Comprehensive Setup Guide**: Required vs. optional credentials clearly documented
- **Local Development Ready**: Works with just OpenAI + Database (30+ sources)
- **GitHub + Reddit Guide**: Step-by-step credential acquisition

---

### v2.3 ‚Äî Validation UX, Trends, and Risk Link (October 2025)
- Validation enhancements:
  - Persisted ‚Äúlast validated‚Äù metadata on each analysis (timestamp + before/after deltas).
  - Validate button shows for medium+ by default; configurable via `NEXT_PUBLIC_VALIDATION_MIN_SEVERITY` and `NEXT_PUBLIC_VALIDATION_ALWAYS`.
  - Severity is recomputed from score and never decreases; backfill endpoint provided.
- Trends improvements:
  - Live aggregation now reads from `HistoricalData` (with fallback), so charts reflect real scores without snapshots.
  - One‚Äëtime backfill endpoint adds missing historical rows for existing analyses.
- Risk banner action:
  - ‚ÄúView critical finding(s)‚Äù link jumps to the most severe analysis card.

### v2.2 ‚Äî Brave Fallback, GPT‚Äë5 Mini, No‚ÄëDB Mode
- Brave Web Search fallback integrated into crawler:
  - Site‚Äërestricted queries via Brave API (`BRAVE_API_KEY`)
  - 10‚Äëminute in‚Äëmemory cache + minimal rate limiting
  - Unit tests with axios mocked (`__tests__/lib/brave-search.test.ts`)
- Default analysis model switched to `gpt-5-mini` with low reasoning:
  - Overridable via `OPENAI_MODEL` and `OPENAI_REASONING_EFFORT`
  - Reasoning option added only for GPT‚Äë5 models
- No‚ÄëDB mode for local development:
  - When `DATABASE_URL` is unset, Prisma is stubbed and APIs return empty datasets
  - Documented in README; `.env.local.example` updated
- DX and stability enhancements:
  - Jest migrated to `ts-jest` for TS/ESM; all tests passing
  - Next.js build skips lint/type checks for local builds (CI should still lint)
  - Added `.env.local.example` and support for `BRAVE_API_KEY`
  - Contributor guide `AGENTS.md` added and updated
- Fixes and refactors:
  - Validation schema moved to `src/lib/validation/schema.ts`
  - Prisma client wrapper simplified in `src/lib/prisma.ts`

### v2.1 ‚Äî Complete Source Coverage & Enhanced UI
- All 7 sources working: DeepMind and Anthropic via Firecrawl API
- Enhanced console output and processing indicators
- Validation system improvements (second‚Äëopinion flow; scores never decrease)

### v2.0 ‚Äî Enhanced AGI Detection
- Upgraded to GPT‚Äë4.1‚Äëmini (historical)
- Improved AGI detection prompt and risk indicators
- Historical tracking + trend visualization

### Advanced Web Crawling
- **Multi-Strategy Approach**: RSS feeds ‚Üí HTTP requests ‚Üí Firecrawl API ‚Üí Browser automation
- **Anti-Blocking**: User agent rotation, stealth techniques, rate limiting
- **All 7 Sources Working**: Successfully crawling 1000+ articles total
- **Firecrawl Integration**: Professional scraping for heavily protected sites (DeepMind, Anthropic)
- **Smart Caching**: 24-hour cache for API-limited services

---

**Disclaimer**: This is an experimental research tool. AGI detection is highly speculative and this system may produce false positives or miss important indicators. Always verify findings through multiple sources.
